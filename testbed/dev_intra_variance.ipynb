{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from os.path import join, basename, dirname\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from feature_analyzer.index.agent import IndexAgent\n",
    "from feature_analyzer.data_tools.embedding_container import EmbeddingContainer\n",
    "from feature_analyzer.utils.template_parser import InferenceResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/kv_zhao/nist-e2e/feature-analyzer/examples/featobj_mergeV1_D40kv2_RMG/'\n",
    "data_dir = '/Users/kv/workspace/feature-analyzer/features/D40kRM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load from /Users/kv/workspace/nist-e2e/outcomes/MERGE_V0/D40kv2_Rms1m_clean/\n",
      "FP: 23466, FN: 1316, NF: 16, WLM:1203\n"
     ]
    }
   ],
   "source": [
    "#RMG = InferenceResult('/home/kv_zhao/nist-e2e/outcomes/MERGE_V1/D40kv2_RMG_iv1_pv1/')\n",
    "RMG = InferenceResult('/Users/kv/workspace/nist-e2e/outcomes/MERGE_V0/D40kv2_Rms1m_clean/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container:embedding_container created\n",
      "Load embedding container from feat_obj format\n",
      "/Users/kv/workspace/feature-analyzer/features/D40kRM/filename_strings.npy is loaded\n",
      "/Users/kv/workspace/feature-analyzer/features/D40kRM/label_ids.npy is loaded\n",
      "/Users/kv/workspace/feature-analyzer/features/D40kRM/instance_ids.npy is loaded\n",
      "/Users/kv/workspace/feature-analyzer/features/D40kRM/embeddings.npy is loaded\n",
      "/Users/kv/workspace/feature-analyzer/features/D40kRM/label_names.npy is loaded\n",
      "/Users/kv/workspace/feature-analyzer/features/D40kRM/landmarks.npy is loaded\n",
      "/Users/kv/workspace/feature-analyzer/features/D40kRM/probabilities.npy is loaded\n",
      "container size: 10000 -> 120429\n",
      "embedding size: 0 -> 1024\n",
      "probability size: 0 -> 1\n",
      "landmark size: 0 -> 10\n",
      "Reset embedding_container\n",
      "Index Table Created\n",
      "Container initialized.\n"
     ]
    }
   ],
   "source": [
    "container = EmbeddingContainer()\n",
    "container.load(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HNSW Index Agent is initialized with 120429 features\n"
     ]
    }
   ],
   "source": [
    "instance_ids = container.instance_ids\n",
    "all_embeddings = container.get_embedding_by_instance_ids(instance_ids)\n",
    "agent = IndexAgent('HNSW', instance_ids, all_embeddings, distance_measure='ip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== embedding_container ===============\n",
      "embeddings: (120429, 1024)\n",
      "probabilities: (120429, 1)\n",
      "landamrks: (120429, 10)\n",
      "internals: instance_ids, label_ids, label_names, filename_strings\n",
      "attributes: source\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "label_ids = list(set(container.label_ids))\n",
    "num_identity = len(label_ids)\n",
    "\n",
    "for label_id in label_ids[2:3]:\n",
    "    same_class_inst_ids = container.get_instance_ids_by_label_ids(label_id)\n",
    "    same_class_embeddings = container.get_embedding_by_instance_ids(same_class_inst_ids)\n",
    "    class_center_embedding = np.mean(same_class_embeddings, axis=0)\n",
    "    class_center_fluct = np.mean(np.std(same_class_embeddings, axis=0))\n",
    "    \n",
    "    # instance\n",
    "    num_inst_same_class = len(same_class_inst_ids)\n",
    "    retrieved_indexes, similarities = agent.search(\n",
    "        same_class_embeddings, top_k = 2 * num_inst_same_class, is_similarity=True)\n",
    "    retrieved_label_ids = container.get_label_by_instance_ids(retrieved_indexes)\n",
    "    hits = retrieved_label_ids == np.asarray([label_id])\n",
    "    \n",
    "    # top k instance\n",
    "    topk_hits = hits[:, :num_inst_same_class]\n",
    "    #np.isin(retrieved_indexes[:, :num_inst_same_class], same_class_inst_ids)\n",
    "    topk_hit_counts = np.sum(topk_hits, axis=1)\n",
    "    topk_miss_counts = np.sum(~topk_hits, axis=1)\n",
    "    topk_purities = topk_hit_counts / num_inst_same_class\n",
    "    topk_same_class_purity = np.mean(topk_purities)\n",
    "    \n",
    "    print(topk_miss_counts, topk_purities, topk_same_class_purity)\n",
    "    \n",
    "    # center\n",
    "    center_retrieved_indexes, center_similarities = agent.search(\n",
    "        class_center_embedding, top_k = 2 * num_inst_same_class, is_similarity=True)\n",
    "    center_retrieved_label_ids = container.get_label_by_instance_ids(center_retrieved_indexes)\n",
    "    center_hits = center_retrieved_label_ids == np.asarray([label_id])\n",
    "    \n",
    "    # top k center\n",
    "    topk_center_hits = center_hits[:, :num_inst_same_class]\n",
    "    topk_center_hit_counts = np.sum(topk_center_hits, axis=1)\n",
    "    topk_center_purities = topk_center_hit_counts / num_inst_same_class\n",
    "    topk_center_same_class_purity = np.mean(topk_center_purities)\n",
    "    \n",
    "    \n",
    "    # top 2k instance\n",
    "    assert hits.shape == retrieved_label_ids.shape\n",
    "    for row, (hit_arr, hit_label_arr) in enumerate(zip(hits, retrieved_label_ids)):\n",
    "        # for each arr, must have negative and positive (self)\n",
    "        #hit_label_arr = retrieved_label_ids[row]\n",
    "        first_neg_id = np.argmax(~hit_arr)\n",
    "        prev_pos_id = first_neg_id - 1\n",
    "        last_pos_id = np.where(hit_arr)[-1][-1]\n",
    "        last_pos_sim = similarities[row, last_pos_id]\n",
    "        first_neg_sim = similarities[row, first_neg_id]\n",
    "        margin = first_neg_sim - last_pos_sim\n",
    "        print(first_neg_id, last_pos_id, hit_arr, hit_label_arr, margin)\n",
    "        \n",
    "    #last_positive_ids = np.asarray(last_positive_ids)\n",
    "    #first_negative_ids = np.asarray(first_negative_ids)\n",
    "    \n",
    "\n",
    "    # if first_neg > last_pos (purity == 1.0) => compute margin\n",
    "    # otherwise, count how many different classes within.\n",
    "    #print('hits', hit_label_ids)\n",
    "    #print('positive ids', positive_ids)\n",
    "    #print('negative ids', negative_ids)\n",
    "\n",
    "    mean_purity += same_class_purity\n",
    "    \n",
    "    counter += 1\n",
    "    #break\n",
    "    \n",
    "end = time.time()\n",
    "hours, rem = divmod(end-start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
